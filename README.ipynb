{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42829b49",
   "metadata": {},
   "source": [
    "# Data Engineering Pipeline Project\n",
    "\n",
    "## Project Goal\n",
    "Automate extraction, loading, and transformation of data to create clean and usable datasets for analysis.\n",
    "\n",
    "## How It Works\n",
    "The pipeline consists of two scripts: `extract_load.py` connects to a live API and loads raw data into a staging table, while `transform.py` cleans and parses the raw data into a structured final table. The transformation script is idempotent, so it can safely be rerun without data duplication or error.\n",
    "\n",
    "## How to Run\n",
    "1. Run `extract_load.py` to fetch and load raw data from the API into the staging database.\n",
    "2. After completion, run `transform.py` to process and clean the staging data into the final table.\n",
    "3. Verify the cleaned data in the final table before use.\n",
    "\n",
    "## Skills Demonstrated\n",
    "\n",
    "### Extraction & Loading (EL)\n",
    "- Fetch data from a live API.\n",
    "- Store raw data in a staging table as a robust pipeline best practice.\n",
    "- Separate loading and transforming steps, following the “digital dump truck” principle.\n",
    "\n",
    "### Transformation (T)\n",
    "- Parse complex JSON from staging data.\n",
    "- Load cleaned data into a structured, final table.\n",
    "- Implement idempotency to ensure safe script reruns without side effects.\n",
    "\n",
    "### Orchestration & Documentation\n",
    "- Design an automated schedule that respects script dependencies.\n",
    "- Write clear, concise documentation enabling others to understand the pipeline quickly.\n",
    "\n",
    "---\n",
    "\n",
    "*Prepared as a portfolio project to demonstrate core data engineering skills.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
